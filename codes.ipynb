{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from keras.applications import ResNet101\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet import preprocess_input, decode_predictions\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Reshape, concatenate\n",
    "from keras.preprocessing.image import img_to_array,load_img\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "import warnings\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory, t_size):\n",
    "    model = ResNet101(weights='imagenet', include_top=False)\n",
    "    model = Model(inputs=model.input, outputs=model.get_layer('conv5_block3_out').output)\n",
    "    # print(model.summary())\n",
    "    features_all = []\n",
    "    for name in tqdm(os.listdir(directory)[t_size-1000:t_size]):\n",
    "        img = load_img(directory+'/'+name,target_size=(224,224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        img_array = img_array.reshape((1, *img_array.shape))\n",
    "        features = model.predict(img_array, verbose=0)\n",
    "        patch_size = 14\n",
    "        features_reshaped = features.reshape((patch_size * patch_size, -1))\n",
    "        features_all.append(features_reshaped)\n",
    "    return np.array(features_all)\n",
    "\n",
    "directory ='./flickr30k_images/flickr30k_images'\n",
    "for t_size in range(1000, 31000, 1000):\n",
    "    patch_features = extract_features(directory, t_size)\n",
    "    with open('./Extracted_Features/resnet_features_'+str(round(t_size/1000))+'.pkl','wb') as f: \n",
    "        pickle.dump(patch_features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"./yolov8x.pt\")       # for detection\n",
    "base_model = ResNet101(weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "def extract_object_features(image, boxes, scores, top_n=3, output_dim=512):\n",
    "    # Select top N predictions\n",
    "    selected_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "\n",
    "    object_features = []\n",
    "    for idx in (selected_indices):\n",
    "        box = boxes[idx]\n",
    "\n",
    "        # Extract region of interest (ROI) based on the bounding box\n",
    "        x, y, w, h = map(int, box)\n",
    "        roi = image[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize ROI to match ResNet50 input size\n",
    "        roi = cv2.resize(roi, (224, 224))\n",
    "\n",
    "        # Preprocess input for ResNet50\n",
    "        roi = preprocess_input(np.expand_dims(roi, axis=0))\n",
    "\n",
    "        # Use a pre-trained ResNet50 model to extract features\n",
    "        x = base_model(roi)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(output_dim, activation='relu')(x)\n",
    "\n",
    "        # Extracted features as a 1D vector\n",
    "        object_vector = x.numpy().flatten()\n",
    "\n",
    "        object_features.append(object_vector)\n",
    "    if len(selected_indices) < top_n:\n",
    "        for i in range(top_n - len(selected_indices)):\n",
    "            try:\n",
    "                object_features.append(np.zeros(object_vector.shape))\n",
    "            except:\n",
    "                object_features.append(np.zeros((512,)))\n",
    "\n",
    "    return np.array(object_features)\n",
    "\n",
    "# Example usage\n",
    "directory ='./flickr30k_images/flickr30k_images'\n",
    "for t_size in range(19000, 31000, 1000):\n",
    "    all_object_features = []\n",
    "    for name in tqdm(os.listdir(directory)[t_size-1000:t_size]):\n",
    "        image_path = directory+'/'+name\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        results = model.predict(image, verbose = False)\n",
    "        boxes = [list(np.array((b[:4]))) for b in results[0].boxes.data]\n",
    "        scores = list(np.array(results[0].boxes.conf))\n",
    "        classes = list(np.array(results[0].boxes.cls))\n",
    "\n",
    "        boxes = list(np.array(boxes)[np.unique(classes, return_index=True)[1]])\n",
    "        scores = list(np.array(scores)[np.unique(classes, return_index=True)[1]])\n",
    "\n",
    "        # Extract object features\n",
    "        object_features = extract_object_features(image, boxes, scores)\n",
    "        all_object_features.append(object_features)\n",
    "\n",
    "    # Convert the list of object features to a NumPy array\n",
    "    all_object_features_array = np.array(all_object_features)\n",
    "    with open('./Extracted_Features/yolo_features_'+str(round(t_size/1000))+'.pkl','wb') as f: \n",
    "        pickle.dump(all_object_features_array, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Extracted_Features/resnet_features_'+str(1)+'.pkl','rb') as f: \n",
    "    patch_features = pickle.load(f)\n",
    "with open('./Extracted_Features/yolo_features_'+str(1)+'.pkl','rb') as f: \n",
    "    all_object_features_array = pickle.load(f)\n",
    "\n",
    "for i in range(2,8):\n",
    "    with open('./Extracted_Features/resnet_features_'+str(i)+'.pkl','rb') as f: \n",
    "        patch_features = np.concatenate((patch_features, pickle.load(f)))\n",
    "\n",
    "    with open('./Extracted_Features/yolo_features_'+str(i)+'.pkl','rb') as f: \n",
    "        all_object_features_array = np.concatenate((all_object_features_array, pickle.load(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = np.concatenate((patch_features, all_object_features_array), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>two young guys with shaggy hair look at their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>two young , white males are outside near many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>a man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34995</th>\n",
       "      <td>2570559405.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>three dogs are behind a rusty fence as one bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34996</th>\n",
       "      <td>2570559405.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>two dogs are looking through a rusty wire fence .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34997</th>\n",
       "      <td>2570559405.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>two dogs are in a fence and one is barking .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34998</th>\n",
       "      <td>2570559405.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>the dogs are behind the fence .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34999</th>\n",
       "      <td>2570559405.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>two dogs bark behind a fence .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34999 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_name comment_number  \\\n",
       "0      1000092795.jpg              0   \n",
       "1      1000092795.jpg              1   \n",
       "2      1000092795.jpg              2   \n",
       "3      1000092795.jpg              3   \n",
       "4      1000092795.jpg              4   \n",
       "...               ...            ...   \n",
       "34995  2570559405.jpg              0   \n",
       "34996  2570559405.jpg              1   \n",
       "34997  2570559405.jpg              2   \n",
       "34998  2570559405.jpg              3   \n",
       "34999  2570559405.jpg              4   \n",
       "\n",
       "                                                 comment  \n",
       "0      two young guys with shaggy hair look at their ...  \n",
       "1      two young , white males are outside near many ...  \n",
       "2       two men in green shirts are standing in a yard .  \n",
       "3           a man in a blue shirt standing in a garden .  \n",
       "4                two friends enjoy time spent together .  \n",
       "...                                                  ...  \n",
       "34995  three dogs are behind a rusty fence as one bar...  \n",
       "34996  two dogs are looking through a rusty wire fence .  \n",
       "34997       two dogs are in a fence and one is barking .  \n",
       "34998                    the dogs are behind the fence .  \n",
       "34999                     two dogs bark behind a fence .  \n",
       "\n",
       "[34999 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_df = pd.read_csv('./flickr30k_images/results.csv', sep='|')\n",
    "caption_df['comment'] = caption_df['comment'].str.lower()\n",
    "caption_df = caption_df.dropna()\n",
    "caption_df = caption_df[caption_df['image_name'].isin(os.listdir(directory)[:7000])]\n",
    "caption_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 10261\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for txt in caption_df['comment']:\n",
    "   vocabulary.extend(txt.split())\n",
    "print('Vocabulary Size: %d' % len(set(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34999it [00:06, 5440.62it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text_original):\n",
    "   text_no_punctuation = text_original.translate(string.punctuation)\n",
    "   return(text_no_punctuation)\n",
    "\n",
    "def remove_single_character(text):\n",
    "   text_len_more_than1 = \"\"\n",
    "   for word in text.split():\n",
    "       if len(word) > 1:\n",
    "           text_len_more_than1 += \" \" + word\n",
    "   return(text_len_more_than1)\n",
    "\n",
    "def remove_numeric(text):\n",
    "   text_no_numeric = \"\"\n",
    "   for word in text.split():\n",
    "       isalpha = word.isalpha()\n",
    "       if isalpha:\n",
    "           text_no_numeric += \" \" + word\n",
    "   return(text_no_numeric)\n",
    "\n",
    "def text_clean(text_original):\n",
    "   text = remove_punctuation(text_original)\n",
    "   text = remove_single_character(text)\n",
    "   text = remove_numeric(text)\n",
    "   return(text)\n",
    "\n",
    "for i, caption in tqdm(enumerate(caption_df['comment'].values)):\n",
    "   newcaption = text_clean(caption)\n",
    "   caption_df[\"comment\"].iloc[i] = newcaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Vocabulary Size: 9596\n"
     ]
    }
   ],
   "source": [
    "clean_vocabulary = []\n",
    "for txt in caption_df['comment'].values:\n",
    "   clean_vocabulary.extend(txt.split())\n",
    "print('Clean Vocabulary Size: %d' % len(set(clean_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df['comment'] = 'startseq ' + caption_df['comment']+ ' endseq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = caption_df[['image_name', 'comment']].groupby('image_name')['comment'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  9599\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input,Dense,LSTM,Embedding,Dropout,Flatten\n",
    "\n",
    "def to_list(descriptions):\n",
    "  all_desc_list = []\n",
    "  for k,v in descriptions.items():\n",
    "    for desc in v:\n",
    "      all_desc_list.append(desc)\n",
    "  return all_desc_list\n",
    "\n",
    "def tokenization(descriptions):\n",
    "  # list of all the descriptions\n",
    "  all_desc_list = to_list(descriptions)  \n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(all_desc_list)\n",
    "  return tokenizer\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = tokenization(train_descriptions)\n",
    "\n",
    "# word index is the dictionary /mappings of word-->integer\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print('Vocab size: ',vocab_size)\n",
    "\n",
    "def max_length(descriptions):\n",
    "  all_desc_list = to_list(descriptions)\n",
    "  return (max(len(x.split()) for x in all_desc_list))\n",
    "\n",
    "\n",
    "def create_sequences(tokenizer,desc_list,max_len,photo):\n",
    "  X1,X2,y = [],[],[]\n",
    "  # X1 will contain photo\n",
    "  # X2 will contain current sequence\n",
    "  # y will contain one hot encoded next word\n",
    "\n",
    "  for desc in desc_list:\n",
    "    # tokenize descriptions\n",
    "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "    for i in range(1,len(seq)):\n",
    "      # out seq is basically the next word in the sentence\n",
    "      in_seq,out_seq = seq[:i],seq[i]\n",
    "      # pad input sequence\n",
    "      in_seq = pad_sequences([in_seq],maxlen=max_len)[0]\n",
    "      # one hot encode output sequence\n",
    "      out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "      X1.append(photo)\n",
    "      X2.append(in_seq)\n",
    "      y.append(out_seq)\n",
    "  return np.array(X1),np.array(X2),np.array(y)\n",
    "\n",
    "# maximum length that a description can have OR the biggest description we are having\n",
    "max_len = max_length(train_descriptions)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {}\n",
    "j = 0\n",
    "for i in os.listdir(directory)[:7000]:\n",
    "    train_features[i] = image_features[j]#.flatten()\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions,photos,tokenizer,max_len):\n",
    "  while 1:\n",
    "    for k,desc_list in descriptions.items():\n",
    "      photo = photos[k]\n",
    "      in_img,in_seq,out_seq = create_sequences(tokenizer,desc_list,max_len,photo)\n",
    "      yield[[in_img,in_seq],out_seq]\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    # image features extractor model\n",
    "    inputs1 = Input(shape=(199,512,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe1 = Flatten()(fe1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    " \n",
    "    # input sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "     # embedding(input_dimension,output_dimension,)\n",
    "     # input dim is always the vocabulary size \n",
    "    # output dimension tells the size of vector space in which the words will be embedded\n",
    "    # mask zero is used when the input itself is 0 then to not confuse it with padded zeros it is used as True\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # decoder model OR output word model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)       [(None, 199, 512)]           0         []                            \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)       [(None, 74)]                 0         []                            \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)        (None, 199, 512)             0         ['input_26[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)    (None, 74, 256)              2457344   ['input_27[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_12 (Flatten)        (None, 101888)               0         ['dropout_24[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)        (None, 74, 256)              0         ['embedding_12[0][0]']        \n",
      "                                                                                                  \n",
      " dense_36 (Dense)            (None, 256)                  2608358   ['flatten_12[0][0]']          \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      " lstm_12 (LSTM)              (None, 256)                  525312    ['dropout_25[0][0]']          \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 256)                  0         ['dense_36[0][0]',            \n",
      "                                                                     'lstm_12[0][0]']             \n",
      "                                                                                                  \n",
      " dense_37 (Dense)            (None, 256)                  65792     ['add_12[0][0]']              \n",
      "                                                                                                  \n",
      " dense_38 (Dense)            (None, 9599)                 2466943   ['dense_37[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31598975 (120.54 MB)\n",
      "Trainable params: 31598975 (120.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 5s 291ms/step - loss: 9.0919\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 8.5230\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 8.5994\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 2s 342ms/step - loss: 7.2333\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 2s 286ms/step - loss: 6.9280\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 6.9397\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 2s 302ms/step - loss: 7.2643\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 2s 332ms/step - loss: 6.6351\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 2s 302ms/step - loss: 7.2947\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 2s 301ms/step - loss: 6.7904\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 2s 298ms/step - loss: 6.4330\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 6.5719\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 2s 354ms/step - loss: 7.1161\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 6.6791\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 2s 301ms/step - loss: 8.7014\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 2s 298ms/step - loss: 6.5475\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 6.5741\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 2s 364ms/step - loss: 6.9116\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 2s 365ms/step - loss: 7.0782\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 2s 331ms/step - loss: 6.7546\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 2s 333ms/step - loss: 9.0800\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 2s 340ms/step - loss: 7.1441\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 2s 289ms/step - loss: 6.8629\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 2s 303ms/step - loss: 6.3811\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 7.0930\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 2s 300ms/step - loss: 6.5877\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 6.7814\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 6.5689\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 7.1722\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 2s 278ms/step - loss: 6.6547\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 6.0755\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 2s 331ms/step - loss: 6.7129\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 2s 277ms/step - loss: 6.5805\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 2s 294ms/step - loss: 6.3232\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 2s 276ms/step - loss: 6.6020\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 2s 296ms/step - loss: 6.1741\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 2s 290ms/step - loss: 6.0768\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 2s 300ms/step - loss: 6.2592\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 5.8533\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 2s 281ms/step - loss: 6.0133\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 2s 271ms/step - loss: 5.8314\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 2s 282ms/step - loss: 5.8934\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 5.9599\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 2s 285ms/step - loss: 6.2130\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 2s 275ms/step - loss: 5.9169\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 6.0695\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 6.4466\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 6.3511\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 2s 298ms/step - loss: 6.0197\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 5.8191\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 2s 270ms/step - loss: 5.7096\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 5.9539\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 2s 285ms/step - loss: 5.4316\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 2s 274ms/step - loss: 5.6683\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 5.9104\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 5.8661\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 2s 277ms/step - loss: 5.9682\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 5.5845\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 2s 299ms/step - loss: 5.6829\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 2s 299ms/step - loss: 6.0393\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 5.9850\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 5.9035\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 2s 288ms/step - loss: 5.7627\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 2s 289ms/step - loss: 5.4716\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 6.1385\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 2s 301ms/step - loss: 5.5413\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 2s 277ms/step - loss: 6.0413\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 6.4034\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 2s 293ms/step - loss: 5.7471\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 2s 272ms/step - loss: 5.7223\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 2s 294ms/step - loss: 6.3283\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 2s 284ms/step - loss: 5.9217\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 2s 274ms/step - loss: 6.0498\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 5.8438\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 2s 289ms/step - loss: 5.6293\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 2s 277ms/step - loss: 5.9683\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 2s 295ms/step - loss: 5.7671\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 5.7522\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 2s 299ms/step - loss: 5.7224\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 6.1859\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 2s 294ms/step - loss: 5.5019\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 2s 297ms/step - loss: 5.4797\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 5.7435\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 5.6928\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 6.0336\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 2s 287ms/step - loss: 6.0269\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 5.7537\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 2s 289ms/step - loss: 5.5425\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 3s 364ms/step - loss: 6.2764\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 2s 304ms/step - loss: 5.7632\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 2s 278ms/step - loss: 5.6323\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 2s 295ms/step - loss: 5.6838\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 2s 284ms/step - loss: 5.7845\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 2s 275ms/step - loss: 5.0937\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 2s 272ms/step - loss: 5.6128\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 5.4311\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 2s 283ms/step - loss: 5.7475\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 5.7984\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 2s 294ms/step - loss: 5.8422\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 5.5761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x116770e2d30>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model(vocab_size,max_len)\n",
    "# epochs = 2\n",
    "steps = len(train_descriptions)/1000\n",
    "# for i in range(epochs):\n",
    "generator = data_generator(train_descriptions,train_features,tokenizer,max_len)\n",
    "model.fit_generator(generator,epochs=100,steps_per_epoch=steps,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "def int2word(tokenizer,integer):\n",
    "  for word,index in tokenizer.word_index.items():\n",
    "    if index==integer:\n",
    "      return word\n",
    "  return None\n",
    "\n",
    "def predict_desc(model,tokenizer,photo,max_len):\n",
    "  in_seq = 'startseq'\n",
    "  for i in range(max_len):\n",
    "    seq = tokenizer.texts_to_sequences([in_seq])[0]\n",
    "    seq = pad_sequences([seq],maxlen=max_len)\n",
    "    y_hat = model.predict([photo, seq],verbose=0)\n",
    "    y_hat = np.argmax(y_hat)\n",
    "    word = int2word(tokenizer,y_hat)\n",
    "    if word==None:\n",
    "      break\n",
    "    in_seq = in_seq+' '+word\n",
    "    if word=='endseq':\n",
    "      break\n",
    "  return in_seq\n",
    "\n",
    "def evaluate_model(model,descriptions,photos,tokenizer,max_len):\n",
    "  actual,predicted = [],[]\n",
    "  for key,desc in tqdm(descriptions.items()):\n",
    "    y_hat = predict_desc(model,tokenizer,photos[key].reshape(-1,199,512),max_len)\n",
    "    references = [d.split() for d in desc]\n",
    "    actual.append(references)\n",
    "    predicted.append(y_hat.split())\n",
    "  print('BLEU-1: %f' %corpus_bleu(actual,predicted,weights=(1.0,0,0,0)))\n",
    "  print('BLEU-2: %f' %corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0)))\n",
    "  print('BLEU-3: %f' %corpus_bleu(actual,predicted,weights=(0.33,0.33,0.33,0)))\n",
    "  print('BLEU-4: %f' %corpus_bleu(actual,predicted,weights=(0.25,0.25,0.25,0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 81.100000\n",
      "BLEU-2: 65.440000\n",
      "BLEU-3: 49.220000\n",
      "BLEU-4: 39.850000\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model,train_descriptions,train_features,tokenizer,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
